# Awesome-Video-Reasoning-Landscape [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<!-- markdownlint-disable MD033 -->
## The Landscape of Video Reasoning: Tasks, Paradigms and Benchmarks‚Äî An Open-Source Survey

## Overview
This Awesome list systematically curates and tracks the latest research in Video Reasoning, spanning diverse modalities, tasks, and modeling paradigms. Instead of focusing on a single family of models, we break down the landscape from multiple complementary perspectives. Following the emerging taxonomy of the field, we highlight three major paradigms:

- üóíÔ∏è CoT-based Video Reasoning

- üïπÔ∏è CoF-based Video Reasoning

- üåà Interleaved Video Reasoning
  
This repository aims to provide a structured, up-to-date overview of these paradigms, serving as an open-source entry point for researchers exploring the evolving landscape of video reasoning.

## Table of Contents
- [Awesome-Video-Reasoning-Landscape](#awesome-video-reasoning-landscape-)
  - [üòéParadigms](#-paradigms)
    - [üóíÔ∏è CoT-based Video Reasoning](#Ô∏è-cot-based-video-reasoning)
    - [üïπÔ∏è CoF-based Video Reasoning](#Ô∏è-cof-based-video-reasoning)
    - [üåà Interleaved Video Reasoning](#-interleaved-video-reasoning)
  - [‚ú®Ô∏èDatasets](#Ô∏èdatasets)
    - [Pretraining Dataset](#pretraining-dataset)
    - [Training Dataset](#training-dataset)
    - [Benchmark](#benchmark)
  - [üåü Star History](#-star-history)
  - [‚ô•Ô∏è Contributors](#Ô∏è-contributors)

<!-- <small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>`Table of contents generated with markdown-toc`</a></i></small> -->

## üòé Paradigms

### üïπÔ∏è CoT-based Video Reasoning

<!-- Á¨¶Âè∑:
‚àö ‚úì
x ‚úó

     ÂæΩÁ´†
         arxiv: https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic
         conference: https://img.shields.io/badge/CVPR-2024-blue.svg?style=plastic
         huggingface checkpoint:![hf_checkpoint](https://img.shields.io/badge/ü§ó-Checkpoints-9C276A.svg)]()
         modelscope
         github model zoos: [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)]()
-->

<!-- Ê®°ÁâàÔºö
|** ** [![arXiv](https://img.shields.io/badge/arXiv-[]-b31b1b.svg?style=plastic)](https://arxiv.org/abs/[])|** ** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://om-cat.github.io.)|unreleased|‚úì|‚úì|‚úì|‚úì|
 -->

 *The last four columns represent the input modalities supported by the model.

| Title                                                                                                                                                                                                             |                                                                                               Model                                                                                               |                                                                                    Checkpoint                                                                                    |   Text   |  Image  |  Audio  |  Video  |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------: | :------: | :------: | :------: |
| **OMCAT: Omni Context Aware Transformer** [![arXiv](https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2410.12109)                                                   |                           **OMCAT** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://om-cat.github.io.)                           |                                                                                    unreleased                                                                                    | &#10003; | &#10003; | &#10003; | &#10003; |
| **Baichuan-Omni Technical Report** [![arXiv](https://img.shields.io/badge/arXiv-2410.08565-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2410.08565)                                                          |           **Baichuan-Omni** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/westlake-baichuan-mllm/bc-omni)           |                             [![hf_checkpoint](https://img.shields.io/badge/ü§ó-Unreleased-9C276A.svg)](https://github.com/westlake-baichuan-mllm/bc-omni)                             | &#10003; | &#10003; | &#10003; | &#10003; |
| **VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs** [![arXiv](https://img.shields.io/badge/arXiv-2406.07476-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2406.07476) | **VideoLLaMA 2** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/DAMO-NLP-SG/VideoLLaMA2) |                 [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)](https://github.com/DAMO-NLP-SG/VideoLLaMA2#earth_americas-model-zoo)                 | &#10003; | &#10003; | &#10003; | &#10003; |
| **GroundingGPT:Language Enhanced Multi-modal Grounding Model** [![arXiv](https://img.shields.io/badge/ACL-2024-blue.svg?style=plastic)](https://arxiv.org/abs/2401.06071)                        |                 **GroundingGPT** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/lzw-lzw/GroundingGPT)                 | [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)](https://github.com/TXH-mercury/VAST#download--vast-models--and-captioners-for-labeling-your-own-data) | &#10003; | &#10003; | &#10003; | &#10003; |
| **VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset** [![NeurIPS](https://img.shields.io/badge/NeurIPS-2023-blue.svg?style=plastic)](http://arxiv.org/abs/2305.18500)                |                       **VAST** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/TXH-mercury/VAST)                       | [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)](https://github.com/TXH-mercury/VAST#download--vast-models--and-captioners-for-labeling-your-own-data) | &#10003; | &#10003; | &#10003; | &#10003; |
| **VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset**[![arXiv](https://img.shields.io/badge/arXiv-2304.08345-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2304.08345) | **VALOR**[![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/TXH-mercury/VALOR) | [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)](https://github.com/TXH-mercury/VALOR#download-checkpoints) | &#10003; | &#10003; | &#10003; | &#10003; |

### üßô CoF-based Video Reasoning

 *The last four columns represent the output modalities supported by the model.

| Title | Model | Checkpoint | Text | Image | Audio | Video |
| :---- | :---: | :--------: | :--: | :---: | :---: | :---: |
| -     |   -   |     -     |  -  |   -   |   -   |   -   |

### üåà Interleaved Video Reasoning

 *The last four columns represent the input & output modalities supported by the model.

| Title                                                                                                                                                                                                                                         |                                                                                                  Model                                                                                                  |                                                                                                                                                                                                                                                                                                                                                         Checkpoint                                                                                                                                                                                                                                                                                                                                                         |   Text   |  Image  |  Audio  |  Video  |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------: | :------: | :------: | :------: |
| **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation** [![arXiv](https://img.shields.io/badge/arXiv-2410.13848-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2410.13848)                               |                           **Janus** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/deepseek-ai/Janus)                           |                                                                                                                                                                                                                                                                                                   [![hf_checkpoint](https://img.shields.io/badge/ü§ó-Janus--1.3B-9C276A.svg)](https://huggingface.co/deepseek-ai/Janus-1.3B)                                                                                                                                                                                                                                                                                                   | &#10003; | &#10003; | &#10007; | &#10007; |
| **Emu3: Next-Token Prediction is All You Need** [![arXiv](https://img.shields.io/badge/arXiv-2409.18869-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2409.18869)                                                                         |                          **emu3** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/baaivision/Emu3)                          | [![hf_checkpoint](https://img.shields.io/badge/ü§ó-Emu3--Chat-9C276A.svg)](https://huggingface.co/BAAI/Emu3-Chat)[![ms_checkpoint](https://img.shields.io/badge/ü§ñ-Emu3--Chat-8A2BE2.svg)](https://huggingface.co/BAAI/Emu3-Chat)<br>[![hf_checkpoint](https://img.shields.io/badge/ü§ó-Emu3--Gen-9C276A.svg)](https://huggingface.co/BAAI/Emu3-Gen)[![ms_checkpoint](https://img.shields.io/badge/ü§ñ-Emu3--Gen-8A2BE2.svg)](https://huggingface.co/BAAI/Emu3-Gen)<br>[![hf_checkpoint](https://img.shields.io/badge/ü§ó-Emu3--VisionTokenizer-9C276A.svg)](https://huggingface.co/BAAI/Emu3-VisionTokenizer)[![ms_checkpoint](https://img.shields.io/badge/ü§ñ-Emu3--VisionTokenizer-8A2BE2.svg)](https://huggingface.co/BAAI/Emu3-VisionTokenizer) | &#10003; | &#10003; | &#10003; | &#10007; |
| **VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation** [![arXiv](https://img.shields.io/badge/arXiv-2409.04429-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2409.04429) | Unreleased | Unreleased | &#10003; | &#10003; | &#10007; | &#10003; |
| **Show-o: One Single Transformer to Unify Multimodal Understanding and Generation** [![arXiv](https://img.shields.io/badge/arXiv-2408.12528-b31b1b.svg?style=plastic)]([https://arxiv.org/abs/2408.12528](https://arxiv.org/abs/2408.12528)) |                          **Show-o** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/showlab/Show-o)                          |                                                                                                                                                                                                                                                                                             [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)](https://github.com/showlab/Show-o#hugging-face-models)                                                                                                                                                                                                                                                                                             | &#10003; | &#10003; | &#10007; | &#10007; |
| **Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model** [![arXiv](https://img.shields.io/badge/arXiv-2408.11039-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2408.11039)                                   | **Transfusion** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)]((https://github.com/lucidrains/transfusion-pytorch)) |                                                                                                                                                                                                                                                                                                                                                         unreleased                                                                                                                                                                                                                                                                                                                                                         | &#10003; | &#10003; | &#10007; | &#10007; |
| **VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing**[![nips-2024](https://img.shields.io/badge/NeurIPS-2024-blue.svg?style=plastic)](https://is.gd/aGu0VV) | **VITRON** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/SkyworkAI/Vitron) | [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)](https://github.com/SkyworkAI/Vitron/blob/main/checkpoints/README.md#checkpoints-preparation) | &#10003; | &#10003; | &#10003; | &#10007; |

## ‚ú®Ô∏èDatasets

### Pretraining Dataset

### Training Dataset

<!-- Ê®°ÁâàÔºö
|**[Êï∞ÊçÆÈõÜÂêçÂ≠ó]**|**[paperÂêçÁß∞]**|ÈìæÊé•|‚úó|‚úó|‚úì|ÊèèËø∞|
 -->

| Dataset Name       |                                                                          Paper                                                                          |                                                                          Link                                                                          | Audio-Image-Text | Speech-Video-Text | Audio-Video-Text |                                                                            Detail                                                                            |
| :----------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------: | :--------------: | :---------------: | :--------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------: |
| **OCTAV** |                                                      **OMCAT: Omni Context Aware Transformer** [![arXiv](https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2410.12109)                                                          |                                                          [unreleased](https://om-cat.github.io.)                                                          |     &#10007;     |     &#10007;     |     &#10003;     | **OCTAV-ST** has**127,507** unique videos with single QA pairs;<br>**OCTAV-MT** **25,457** unique videos with a total of **180,916** QA pairs. |
| **VAST-27M** | **VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset** [![NeurIPS](https://img.shields.io/badge/NeurIPS-2023-blue.svg?style=plastic)](http://arxiv.org/abs/2305.18500) | **VAST** [![project_repo](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/TXH-mercury/VAST) |     &#10007;     |     &#10007;     |     &#10003;     |                                                     **27M** Clips;<br>**297M** Captions.                                                     |
|**VALOR-1M**|**VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset**[![arXiv](https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2304.08345)|ÈìæÊé•|‚úó|‚úó|‚úì|ÊèèËø∞|

### Benchmark

| Name          | Paper                                                        | Link                                                         | SI:Text  | SI:Image | SI:Audio | SI:Video | SO:Text  | Detail                                                       |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- | -------- | -------- | -------- | -------- | ------------------------------------------------------------ |
| **OmnixR**    | **OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities** [![arXiv](https://img.shields.io/badge/arXiv-2410.12219-b31b1b.svg?style=plastic)](http://arxiv.org/abs/2410.12219) | Unreleased                                                   | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | **$`\text{Omnix}R_\text{synth} `$**:100videos<br>**$`\text{Omnix}R_\text{real} `$**:100videos |
| **OmniBench** | **OmniBench: Towards The Future of Universal Omni-Language Models**[![arXiv](https://img.shields.io/badge/arXiv-2409.15272-b31b1b.svg?style=plastic)](http://arxiv.org/abs/2409.15272) | [![hf_checkpoint](https://img.shields.io/badge/ü§ó-OmniBench-9C276A.svg)](https://huggingface.co/datasets/m-a-p/OmniBench)<br>[![hf_checkpoint](https://img.shields.io/badge/ü§ó-OmniInstruct--v1-9C276A.svg)](https://huggingface.co/datasets/m-a-p/OmniInstruct_v1) | &#10003; | &#10003; | &#10003; | &#10007; | &#10007; |                                                              |
|**VALOR-32K**|**VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset**[![arXiv](https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2304.08345)|ÈìæÊé•|‚úó|‚úó|‚úì|ÊèèËø∞|

## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=LJungang/Awesome-Video-Reasoning-Landscape&type=Date)](https://star-history.com/#LJungang/Awesome-Omni-Large-Models-and-Datasets&Date)

## ‚ô•Ô∏è Contributors

<!--
<a href="https://github.com/LJungang/Awesome-Video-Reasoning-Landscape/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Video-Reasoning-Landscape" />
</a>
 -->

<a href="https://github.comLJungang/Awesome-Video-Reasoning-Landscape/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Video-Reasoning-Landscape" alt="Contributors for Awesome Video Reasoning Landscape"/>
</a>

<!-- markdownlint-enable MD033 -->
