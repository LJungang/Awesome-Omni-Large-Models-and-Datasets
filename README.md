# Awesome-Omni-Large-Models-and-Datasets[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
ğŸ”¥ Omni large models and datasets for understanding and generating multi-modalities.

- [Awesome-Omni-Large-Models-and-Datasets](#awesome-omni-large-models-and-datasets)
    * [ğŸ˜Models](#--models)
    + [ğŸ—’ï¸ Taxonomy](#----taxonomy)
    + [ğŸ•¹ï¸ Modality Understanding](#----modality-understanding)
    + [ğŸ§™ Modality Generation](#---modality-generation)
    + [ğŸŒˆ Unified Model for Understanding and Generating Modalities](#---unified-model-for-understanding-and-generating-modalities)
  * [âœ¨ï¸Datasets](#--datasets)
    + [Pretraining Dataset](#pretraining-dataset)
    + [Training Dataset](#training-dataset)
    + [Benchmark](#benchmark)
  * [ğŸŒŸ Star History](#---star-history)
  * [â™¥ï¸ Contributors](#---contributors)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>


## ğŸ˜Models
### ğŸ—’ï¸ Taxonomy
 <!-- arxiv: [![arXiv](https://img.shields.io/badge/arXiv-2406.09272-b31b1b.svg?style=plastic)]()
  -->
### ğŸ•¹ï¸ Modality Understanding
<!-- ç¬¦å·:
âˆš &#10003;
x &#10007;

     å¾½ç« 
         arxiv: https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic
         conference: https://img.shields.io/badge/CVPR-2024-blue.svg?style=plastic
-->

<!-- æ¨¡ç‰ˆï¼š
|** ** [![arXiv](https://img.shields.io/badge/arXiv-[]-b31b1b.svg?style=plastic)](https://arxiv.org/abs/[])|**OMCAT** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://om-cat.github.io.)|Unpublished|&#10003;|&#10003;|&#10003;|&#10003;|
 -->
 *The last four columns represent the input modalities supported by the model.
|Title|Model|Checkpoint|Text|Image|Audio|Video|
|:---------| :-----: | :-----: | :----: | :-----: |:-----: |:----: |
|**OMCAT: Omni Context Aware Transformer** [![arXiv](https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2410.12109)|**OMCAT** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://om-cat.github.io.)|Unpublished|&#10003;|&#10003;|&#10003;|&#10003;|
|**Baichuan-Omni Technical Report** [![arXiv](https://img.shields.io/badge/arXiv-2410.08565-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2410.08565)|**Baichuan-Omni** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)]([https://om-cat.github.io.](https://github.com/westlake-baichuan-mllm/bc-omni?tab=readme-ov-file))|[![hf_checkpoint](https://img.shields.io/badge/ğŸ¤—-Checkpoints-9C276A.svg)](https://github.com/westlake-baichuan-mllm/bc-omni)|&#10003;|&#10003;|&#10003;|&#10003;|
|**VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs** [![arXiv](https://img.shields.io/badge/arXiv-2406.07476-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2406.07476)|**VideoLLaMA 2** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)]([https://om-cat.github.io.](https://github.com/DAMO-NLP-SG/VideoLLaMA2))|[![hf_checkpoint](https://img.shields.io/badge/ğŸ¤—-Checkpoints-9C276A.svg)](https://huggingface.co/collections/DAMO-NLP-SG/videollama-2-6669b6b6f0493188305c87ed)|&#10003;|&#10003;|&#10003;|&#10003;|
|**VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset** [![NeurIPS](https://img.shields.io/badge/NeurIPS-2023-blue.svg?style=plastic)](http://arxiv.org/abs/2305.18500)|**VAST** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/TXH-mercury/VAST)|[Github Link](https://github.com/TXH-mercury/VAST)|&#10003;|&#10003;|&#10003;|&#10003;|

### ğŸ§™ Modality Generation
 *The last four columns represent the output modalities supported by the model.
|Title|Model|Checkpoint|Text|Image|Audio|Video|
|:---------| :-----: | :-----: | :----: | :-----: |:-----: |:----: |
|-|-|-|-|-|-|-|


### ğŸŒˆ Unified Model for Understanding and Generating Modalities
 *The last four columns represent the input & output modalities supported by the model.
|Title|Model|Checkpoint|Text|Image|Audio|Video|
|:---------| :-----: | :-----: | :----: | :-----: |:-----: |:----: |
|**Show-o: One Single Transformer to Unify Multimodal Understanding and Generation** [![arXiv](https://img.shields.io/badge/arXiv-2408.12528-b31b1b.svg?style=plastic)]([https://arxiv.org/abs/2408.12528](https://arxiv.org/abs/2408.12528))|**Show-o** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/showlab/Show-o)|[![hf_checkpoint](https://img.shields.io/badge/ğŸ¤—-Checkpoints-9C276A.svg)](https://huggingface.co/showlab)|&#10003;|&#10003;|&#10007;|&#10007;|


## âœ¨ï¸Datasets
### Pretraining Dataset

### Training Dataset
<!-- æ¨¡ç‰ˆï¼š
|**[æ•°æ®é›†åå­—]**|é“¾æ¥|&#10007;|&#10007;|&#10003;|æè¿°|
 -->
|Dataset Name|Link|Audio-Image-Text|Speech-Video-Text|Audio-Video-Text|Detail|
|:--------- |:----:| :-----: |:-----: |:----:|:----:|
|**OCTAV**|[Unpublished](https://om-cat.github.io.)|&#10007;|&#10007;|&#10003;|OCTAV-ST has **127,507** unique videos with single QA pairs;<br>OCTAV-MT **25,457** unique videos with a total of **180,916** QA pairs.|
|**VAST-27M**|**VAST** [![](https://img.shields.io/badge/Github-181717?style=plastic&logo=github&logoColor=white)](https://github.com/TXH-mercury/VAST)|&#10007;|&#10007;|&#10003;|**27M** Clips;<br>**297M** Captions.|

### Benchmark

## ğŸŒŸ Star History
[![Star History Chart](https://api.star-history.com/svg?repos=LJungang/Awesome-Omni-Large-Models-and-Datasets&type=Date)](https://star-history.com/#LJungang/Awesome-Omni-Large-Models-and-Datasets&Date)

## â™¥ï¸ Contributors
<!--
<a href="https://github.com/LJungang/Awesome-Omni-Large-Models-and-Datasets/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Omni-Large-Models-and-Datasets" />
</a>
 -->
<a href="https://github.comLJungang/Awesome-Omni-Large-Models-and-Datasets/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Omni-Large-Models-and-Datasets" />
</a>
